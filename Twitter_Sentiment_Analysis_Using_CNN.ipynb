{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_Bf2M69-FO77hII8QUJ4hI7Wd8LEZmgQ",
      "authorship_tag": "ABX9TyNfyjtIPpgoxp7G4seGR+yn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hieupc02/Twitter-Sentiment-Analysis-Using-CNN_NLP/blob/main/Twitter_Sentiment_Analysis_Using_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YROxr_owkylx",
        "outputId": "d0aa7ea8-1779-4c4c-e9e1-1113da365df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-16 02:38:55--  https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/eyaler/word2vec-slim/master/GoogleNews-vectors-negative300-SLIM.bin.gz [following]\n",
            "--2023-03-16 02:38:56--  https://media.githubusercontent.com/media/eyaler/word2vec-slim/master/GoogleNews-vectors-negative300-SLIM.bin.gz\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 276467217 (264M) [application/octet-stream]\n",
            "Saving to: ‘GoogleNews-vectors-negative300-SLIM.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>] 263.66M   202MB/s    in 1.3s    \n",
            "\n",
            "2023-03-16 02:39:13 (202 MB/s) - ‘GoogleNews-vectors-negative300-SLIM.bin.gz’ saved [276467217/276467217]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz\n",
        "!gunzip GoogleNews-vectors-negative300-SLIM.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# This is for making some large tweets to be displayed\n",
        "pd.options.display.max_colwidth = 100\n",
        "\n",
        "# I got some encoding issue, I didn't knew which one to use !\n",
        "# This post suggested an encoding that worked!\n",
        "#https://stackoverflow.com/questions/19699367/unicodedecodeerror-utf-8-codec-cant-decode-byte\n",
        "train_data = pd.read_csv(\"/content/train.csv\", encoding='ISO-8859-1',)"
      ],
      "metadata": {
        "id": "1YZHJqtmk3Hp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "06m-pZPKk3wW",
        "outputId": "4f1e4adb-5ef0-46c8-b0fb-c9c4d4f9ebe1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       ItemID  Sentiment  \\\n",
              "0           1          0   \n",
              "1           2          0   \n",
              "2           3          1   \n",
              "3           4          0   \n",
              "4           5          0   \n",
              "...       ...        ...   \n",
              "99984   99996          0   \n",
              "99985   99997          1   \n",
              "99986   99998          0   \n",
              "99987   99999          1   \n",
              "99988  100000          1   \n",
              "\n",
              "                                                                                             SentimentText  \n",
              "0                                                                 is so sad for my APL friend.............  \n",
              "1                                                                         I missed the New Moon trailer...  \n",
              "2                                                                                  omg its already 7:30 :O  \n",
              "3                .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2...  \n",
              "4                                                             i think mi bf is cheating on me!!!       T_T  \n",
              "...                                                                                                    ...  \n",
              "99984                       @Cupcake  seems like a repeating problem   hope you're able to find something.  \n",
              "99985  @cupcake__ arrrr we both replied to each other over different tweets at the same time  , i'll se...  \n",
              "99986                                                                       @CuPcAkE_2120 ya i thought so   \n",
              "99987                                        @Cupcake_Dollie Yes. Yes. I'm glad you had more fun with me.   \n",
              "99988                                                                      @cupcake_kayla haha yes you do   \n",
              "\n",
              "[99989 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f083dd4-0078-43a9-924c-7b4f3fe289d4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ItemID</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL friend.............</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trailer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me!!!       T_T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99984</th>\n",
              "      <td>99996</td>\n",
              "      <td>0</td>\n",
              "      <td>@Cupcake  seems like a repeating problem   hope you're able to find something.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99985</th>\n",
              "      <td>99997</td>\n",
              "      <td>1</td>\n",
              "      <td>@cupcake__ arrrr we both replied to each other over different tweets at the same time  , i'll se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99986</th>\n",
              "      <td>99998</td>\n",
              "      <td>0</td>\n",
              "      <td>@CuPcAkE_2120 ya i thought so</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99987</th>\n",
              "      <td>99999</td>\n",
              "      <td>1</td>\n",
              "      <td>@Cupcake_Dollie Yes. Yes. I'm glad you had more fun with me.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99988</th>\n",
              "      <td>100000</td>\n",
              "      <td>1</td>\n",
              "      <td>@cupcake_kayla haha yes you do</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99989 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f083dd4-0078-43a9-924c-7b4f3fe289d4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2f083dd4-0078-43a9-924c-7b4f3fe289d4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2f083dd4-0078-43a9-924c-7b4f3fe289d4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "embed = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300-SLIM.bin', binary=True)"
      ],
      "metadata": {
        "id": "0uRGiXIBnY4h"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vocabulary used by the embedding\n",
        "\n",
        "# dictionary for efficient search\n",
        "embed_vocab = {}\n",
        "for word in embed.vocab:\n",
        "    embed_vocab[word] = True"
      ],
      "metadata": {
        "id": "4KbfVn86nebk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = train_data['SentimentText']\n",
        "tweets_words = []\n",
        "tweets_vocab = {}\n",
        "for tweet in tweets:\n",
        "    # Don't include words that can't actually be mapped to a vector using embeddings\n",
        "    words = tweet.split()\n",
        "    filtered_words = []\n",
        "    for word in words:\n",
        "        tweets_vocab[word] = True\n",
        "        if embed_vocab.get(word) is not None:\n",
        "            filtered_words.append(word)\n",
        "    tweets_words.append(filtered_words)"
      ],
      "metadata": {
        "id": "mrRzpjM_ng4h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We check how many words from the dataset's vocabulary\n",
        "# are found in the embedding vocab\n",
        "\n",
        "def vocab_coverage(compared, base):\n",
        "    hit, miss = 0, 0\n",
        "    for word in compared:\n",
        "        if base.get(word) is not None:\n",
        "            hit += 1\n",
        "        else:\n",
        "            miss += 1\n",
        "\n",
        "    print(\"{} words were found\".format(hit))\n",
        "    print(\"{} words weren't found\".format(miss))\n",
        "    \n",
        "vocab_coverage(tweets_vocab, embed_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTA4X-6Pnj11",
        "outputId": "5edd1d3e-ccdd-4830-ff4d-886fb165e0fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33676 words were found\n",
            "150015 words weren't found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We check the distribution of length of tweets after word filtering\n",
        "from collections import Counter\n",
        "\n",
        "len_counts = Counter([len(tweet) for tweet in tweets_words])\n",
        "\n",
        "print(len_counts)\n",
        "pd.DataFrame([len(tweet) for tweet in tweets_words]).hist(bins=list(range(30)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "i4vvUUYHnmsJ",
        "outputId": "d6b54691-21f8-46ae-8dbb-753ea0e09bcc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({5: 7170, 4: 7157, 6: 6788, 7: 6511, 3: 6390, 8: 5958, 9: 5788, 10: 5363, 2: 5277, 11: 5042, 12: 4781, 13: 4464, 14: 4107, 15: 3901, 16: 3753, 1: 3407, 17: 3199, 18: 2756, 19: 2140, 0: 1957, 20: 1550, 21: 1052, 22: 654, 23: 426, 24: 231, 25: 83, 26: 44, 27: 26, 28: 6, 29: 5, 80: 1, 40: 1, 30: 1})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<AxesSubplot:title={'center':'0'}>]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_words_idx = []\n",
        "for tweet in tweets_words:\n",
        "    indexs = []\n",
        "    for word in tweet:\n",
        "        idx = embed.vocab[word].index\n",
        "        indexs.append(idx)\n",
        "    tweets_words_idx.append(indexs)\n",
        "\n",
        "print(\"An example tweet: {}\".format(tweets_words_idx[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CoUtovGnqkQ",
        "outputId": "7192944c-757c-4524-ac33-baa84a989325"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An example tweet: [3, 80, 3949, 1, 119, 43610]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = np.array(train_data['Sentiment'])\n",
        "def prepare_data(all_tweets, all_labels, max_words):\n",
        "    labels = []\n",
        "    tweets = []\n",
        "    for i in range(len(all_tweets)):\n",
        "        tweet = all_tweets[i]\n",
        "        if not tweet:\n",
        "            continue\n",
        "        diff = max_words - len(tweet)\n",
        "        if diff > 0: # need to pad\n",
        "            tweet = [0 for j in range(diff)] + tweet\n",
        "        elif diff < 0:\n",
        "            tweet = tweet[: max_words]\n",
        "        tweets.append(tweet)\n",
        "        labels.append(all_labels[i])\n",
        "    return (np.array(tweets), np.array(labels))\n",
        "\n",
        "tweets, labels = prepare_data(tweets_words_idx, all_labels, 20)\n",
        "assert len(tweets) == len(labels)\n",
        "        "
      ],
      "metadata": {
        "id": "R9YNpyc0nt4V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG0J7FjAcr2d",
        "outputId": "87bd8d42-0c14-484b-a217-949bddbcc952"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train/val/test\n",
        "import random\n",
        "\n",
        "# to reproduce\n",
        "random.seed(73)\n",
        "\n",
        "ds_size = len(tweets)\n",
        "train_ds_ratio = 0.7\n",
        "idxs = list(range(ds_size))\n",
        "random.shuffle(idxs)\n",
        "\n",
        "end_train = int(ds_size * 0.7)\n",
        "train_idxs = idxs[:end_train]\n",
        "remaining_idxs = idxs[end_train:]\n",
        "test_end = int(len(remaining_idxs) * 0.5)\n",
        "test_idxs = remaining_idxs[:test_end]\n",
        "val_idxs = remaining_idxs[test_end:]\n",
        "\n",
        "train_data, train_label = tweets[train_idxs], labels[train_idxs]\n",
        "test_data, test_label = tweets[test_idxs], labels[test_idxs]\n",
        "val_data, val_label = tweets[val_idxs], labels[val_idxs]"
      ],
      "metadata": {
        "id": "abCtS2jZnzjR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "datasets = {\n",
        "    'train': TensorDataset(torch.from_numpy(train_data), torch.from_numpy(train_label)),\n",
        "    'val': TensorDataset(torch.from_numpy(val_data), torch.from_numpy(val_label)),\n",
        "    'test': TensorDataset(torch.from_numpy(test_data), torch.from_numpy(test_label)),\n",
        "}\n",
        "\n",
        "dataloaders = {\n",
        "    'train': DataLoader(datasets['train'], batch_size=batch_size),\n",
        "    'val': DataLoader(datasets['val'], batch_size=batch_size),\n",
        "    'test': DataLoader(datasets['test'], batch_size=batch_size),\n",
        "}"
      ],
      "metadata": {
        "id": "LRyp_8SKn2Ud"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Huấn luyện và đào tạo**\n"
      ],
      "metadata": {
        "id": "CaThNnD1VWtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "metadata": {
        "id": "PnM7mTfdn3Dq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dae22b6-ba84-48af-d713-f9475f0be2af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SentimentCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The embedding layer + CNN model that will be used to perform sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_model, vocab_size, output_size, embedding_dim,\n",
        "                 num_filters=100, kernel_sizes=[3, 4, 5], freeze_embeddings=True, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentCNN, self).__init__()\n",
        "\n",
        "        # set class vars\n",
        "        self.num_filters = num_filters\n",
        "        self.embedding_dim = embedding_dim\n",
        "        \n",
        "        # 1. embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # set weights to pre-trained\n",
        "        self.embedding.weight = nn.Parameter(torch.from_numpy(embed_model.vectors)) # all vectors\n",
        "        # (optional) freeze embedding weights\n",
        "        if freeze_embeddings:\n",
        "            self.embedding.requires_grad = False\n",
        "        \n",
        "        # 2. convolutional layers\n",
        "        self.convs_1d = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_filters, (k, embedding_dim), padding=(k-2, 0)) \n",
        "            for k in kernel_sizes])\n",
        "        \n",
        "        # 3. final, fully-connected layer for classification\n",
        "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size) \n",
        "        \n",
        "        # 4. dropout and sigmoid layers\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "    \n",
        "    def conv_and_pool(self, x, conv):\n",
        "        \"\"\"\n",
        "        Convolutional + max pooling layer\n",
        "        \"\"\"\n",
        "        # squeeze last dim to get size: (batch_size, num_filters, conv_seq_length)\n",
        "        # conv_seq_length will be ~ 200\n",
        "        x = F.relu(conv(x)).squeeze(3)\n",
        "        \n",
        "        # 1D pool over conv_seq_length\n",
        "        # squeeze to get size: (batch_size, num_filters)\n",
        "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x_max\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines how a batch of inputs, x, passes through the model layers.\n",
        "        Returns a single, sigmoid-activated class score as output.\n",
        "        \"\"\"\n",
        "        # embedded vectors\n",
        "        embeds = self.embedding(x) # (batch_size, seq_length, embedding_dim)\n",
        "        # embeds.unsqueeze(1) creates a channel dimension that conv layers expect\n",
        "        embeds = embeds.unsqueeze(1)\n",
        "        \n",
        "        # get output of each conv-pool layer\n",
        "        conv_results = [self.conv_and_pool(embeds, conv) for conv in self.convs_1d]\n",
        "        \n",
        "        # concatenate results and add dropout\n",
        "        x = torch.cat(conv_results, 1)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # final logit\n",
        "        logit = self.fc(x) \n",
        "        \n",
        "        # sigmoid-activated --> a class score\n",
        "        return self.sig(logit)\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "DXA-cSrVn5ur"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "\n",
        "vocab_size = len(embed_vocab)\n",
        "output_size = 1 # binary class (1 or 0)\n",
        "embedding_dim = len(embed['for']) # 300-dim vectors\n",
        "num_filters = 100\n",
        "kernel_sizes = [3, 4, 5]\n",
        "\n",
        "net = SentimentCNN(embed, vocab_size, output_size, embedding_dim,\n",
        "                   num_filters, kernel_sizes)\n",
        "\n",
        "print(net)"
      ],
      "metadata": {
        "id": "RDyZQG-mn81q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe3e956a-4a5e-4dfc-e5ba-3295f1f2d03b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentCNN(\n",
            "  (embedding): Embedding(299567, 300)\n",
            "  (convs_1d): ModuleList(\n",
            "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
            "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n",
            "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n",
            "  )\n",
            "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "# training loop\n",
        "def train(net, train_loader, valid_loader, epochs, print_every=100):\n",
        "\n",
        "    # move model to GPU, if available\n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0 # for printing\n",
        "    min_loss = np.Inf\n",
        "    # train for some number of epochs\n",
        "    net.train()\n",
        "    for e in range(epochs):\n",
        "\n",
        "        # batch loop\n",
        "        for inputs, labels in train_loader:\n",
        "            counter += 1\n",
        "\n",
        "            if(train_on_gpu):\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output = net(inputs)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for inputs, labels in valid_loader:\n",
        "\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                    output = net(inputs)\n",
        "                    val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                if np.mean(val_losses) < min_loss:\n",
        "                    min_loss = np.mean(val_losses)\n",
        "                    torch.save(net.state_dict(), \"model.pth\")\n",
        "                    print(\"New val loss... saving model\")\n",
        "                \n",
        "                net.train()\n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "metadata": {
        "id": "3qdkK-kcn_Fn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training params\n",
        "\n",
        "epochs = 5 # this is approx where I noticed the validation loss stop decreasing\n",
        "print_every = 500\n",
        "\n",
        "train(net, dataloaders['train'], dataloaders['val'], epochs, print_every=print_every)"
      ],
      "metadata": {
        "id": "-LfDWLI7oCRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44109b13-1953-43e1-f565-8cc4faf42c44"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New val loss... saving model\n",
            "Epoch: 1/5... Step: 500... Loss: 0.311315... Val Loss: 0.650898\n",
            "New val loss... saving model\n",
            "Epoch: 1/5... Step: 1000... Loss: 0.784586... Val Loss: 0.574761\n",
            "Epoch: 2/5... Step: 1500... Loss: 0.276878... Val Loss: 0.754105\n",
            "Epoch: 2/5... Step: 2000... Loss: 0.272420... Val Loss: 0.690113\n",
            "Epoch: 3/5... Step: 2500... Loss: 0.225262... Val Loss: 0.883953\n",
            "Epoch: 3/5... Step: 3000... Loss: 0.097962... Val Loss: 0.830467\n",
            "Epoch: 4/5... Step: 3500... Loss: 0.223522... Val Loss: 0.983375\n",
            "Epoch: 4/5... Step: 4000... Loss: 0.112720... Val Loss: 0.980194\n",
            "Epoch: 5/5... Step: 4500... Loss: 0.075105... Val Loss: 1.308970\n",
            "Epoch: 5/5... Step: 5000... Loss: 0.127307... Val Loss: 1.242924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.load_state_dict(torch.load(\"model.pth\"))"
      ],
      "metadata": {
        "id": "Dud5ULZPoELp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc069257-68ce-4cf6-ba2e-72c76c330b9e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test**"
      ],
      "metadata": {
        "id": "QlJosIPWoGV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "\n",
        "net.eval()\n",
        "# lặp lại dữ liệu thử nghiệm\n",
        "for inputs, labels in dataloaders['test']:\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "   # nhận kết quả đầu ra dự đoán\n",
        "    output = net(inputs)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct / len(dataloaders['test'].dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "o6Y9_zyAoIBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ebdba0e-e2c3-4e88-f72a-2325f1e92ae1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.583\n",
            "Test accuracy: 0.730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://github.com/hieupc02/Twitter-Sentiment-Analysis-Using-CNN_NLP.git\n",
        "!git branch -M main \n",
        "!git push -u origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kVES31HblyP",
        "outputId": "28ab1c6b-e16f-4228-eac3-a655acaf68e2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    }
  ]
}